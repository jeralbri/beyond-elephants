---
title: "Chapter 1"
author: "Jeremy Albright"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(knitr)

```

# Introduction

There may come a time in a young person's life when she begins to question her understanding of the world.  Tired of falling back on the simple heuristics that had once seemed good enough, she soon finds herself asking the deeper questions.  What is an eigenvalue?  How can one intuit back-propagation?  Why does it take so long for a generalized linear mixed model to converge?  And what, exactly, does it mean to find convergence at all?  It is for such people, desiring that their model results are more than just shadows on a cave wall, that this book is written.

In the 20th century, the field of quantitative modeling was mostly concentrated among people who pursued advanced degrees in the natural and social sciences, though a few intrepid MBAs would attempt apply basic statistics to study consumer behavior or predict business trends.  A small percentage of this group would go all in and learn the theory underlying econometric modeling or even game theory.  Most graduate students, research faculty, and workers in industry, however, were satisfied to complete a required sequence in statistics plus maybe - if the program was better than average - basic calculus and an overview matrices.  They then went about their careers happily building models that they believed to be explanatory or predictive. With luck, a journal would eventually publish their results following a review by cantankerous peers who got around to reading the mansucript after a few months.  

Among those with more advanced training, there was always concern that the less sophisticated analysts were merely pointing and clicking their way through SPSS until a magical p-value less than 0.05 appeared, even though the researcher had only a superficial understanding of the data and the appropriateness of the methodology.  Because software became so easy to use, "Garbage-In Garbage-Out" results became ubiquitous.  Indeed, as has recently been demonstrated, many of these published "scientific" findings have failed to replicate or generate accurate out-of-sample predictions. While this has rarely been due to outright fraud on behalf of the investigators, a lack of understanding of the first principles underlying the various quantitative models is a big source of the problem.  Researchers who know software but don't understand, say, sampling theory or asymptotics, are often afraid to look beyond the $p$-value spit out by their computer.  While everybody knows deep down in their hearts that $p$-values are problematic, not many researchers seem to be able to explain _why_ they should not be the determining factor in deciding whether a finding is "significant" in the colloquial sense of the word.  

The modeling options available to researchers in academia and industry have exploded in number in the 21st century.  Statistics has re-oriented itself around the "causal inference" paradigm, eschewing 20th-century generalized linear models in favor of approaches that claim to better isolate associations that are causal in nature.  The field of "data science", which overlaps with statistics but also includes machine learning artificial intelligence, is highly in demand especially in industry, and there is no shortage of traditional and online learning options claiming to teach these skills to motivated students.  The same problem that plagued 20th century research methods, however, has only become more prevalent.  

Data science education tends to emphasize programming, an essential tool for working with the types of high-dimensional, nonlinear models that are used to tell us if a picture contains a cat or a dog or a hot dog.  However, as with 20th century training in statistics, most data science pedagogy skims over the mathematical first principles that allow the models to work. Worse, software to automate machine learning tasks is now being introduced, including point-and-click or drag-and-drop interfaces that do not require the user to know anything about the data or the underlying computations.  The upshot is that  garbage-in garbage-out models are inevitably going to become ubiquitous, and companies that hire self-proclaimed data scientists may quickly get frustrated when the eager young millennial employee cannot troubleshoot a poorly performing deep learning model.

Admittedly, there is a role for the types of applied statisticians and data scientists who let the software do the work without much of an understanding of the underlying algorithms.  These worker bees allow research groups to scale up by providing a workforce capable of implementing previously validated models to a well understood problem.  A company that has found that a regression or ARIMA model performs well in predicting sales only needs somebody to update the model as new data come in.  A principal investigator overseeing a randomized controlled trial may only need a research assistant to run difference in means tests.  But as research questions and data become more complex, the number of opportunities for something to go wrong only increases.  Simply verifying that the data are valid and reliable measures of a phenomenon of interest may be elided, and on top of that inappropriately complex models may be forced on the data.  Spend a week as a consultant trying to explain to a mid-career psychology professor why their mediation model does not prove anything, or telling an investor why artificial intelligence is not a panacea for predicting extraordinarily noisy stock movements, and you will quickly recognize how often results are presented that tell us absolutely nothing about how the world works.

## Why Spend Time on Fundamentals?

There are reasons why first principals are elided or superficially covered in statistics and data science training.  Most Ph.D. candidates want to study something about nature or society, and data-drive models are only a tool used to test hypotheses.  Spending a lot of time on math takes away from time spent investigating what is really of primary interest (though physics and economics students do not seem to have this problem).  The same applies for students of data science, who are expected to learn Python or R programming, methods of cross-validation, random forests, gradient boosted machines, support vector machines, shallow neural networks, and deep learning architectures.  This is its own daunting challenge, despite some for-profit online programs claiming to help you master these in one month (!!!).  Packages and high-level APIs exist to abstract over the linear algebra and numeric optimization happening behind the scenes, so a quick picture showing a derivative and a vector in 2d space is thought to suffice for the background math.  The danger, however, is garbage-in, garbage-out.  As evidence, try Googling "quantitative investment firms fail."

There are several reasons to take the time to understand the math behind the models.  First, it is far easier to troubleshoot a bad model. When a complex methodology is imposed on data, it may seem overwhelming to determine why the predictions are failing or expected associations are not appearing.  Analyses of treatment effects in observational data, approached carefully, can reveal that the control and treated cases are so different that it makes no sense to even make the comparison.  A machine learning model may choke because of features that are too highly multicolinear, or too lacking in variability to explain anything, for the model to converge to a meaninful prediction.  Explorations of simple effects within increasing coarsened combinations of other variables, can help the analyst pinpoint "predictors" that do not predict anything.

Second, you learn a couple things when you get to my age.  Like friends don't lie and the latest flashy methodological mansucript may be completely forgotten in five years.  This is especially true in machine learning and AI, where it extraordinarily difficult, if possible at all, to derive analytic results explaining important concepts like sampling uncertainty.  The results presented may look impressive for a particular problem, but it is not worth trying out in a project with real world implications until others have demonstrated that the results generalize.  

Third, it is easier to get creative when the problem is unique. Within statistics, there may not exist a pre-programmed estimator that is appropriate for your research question.  Why not write your own?!?  Knowledge of likelihood functions or Bayesian simulation opens up the research world for answering questions that would otherwise seem intractable.  In the world of AI, finding the best deep learning architecture is a combination of creativity and luck.  I personally do not have any luck (I once scientifically proved, in repeated games of chance with known odds, that I am a loser), so sometimes creativity is all some of us have.


## How to Learn from This Book

One of the great things about math and science is that knowledge is cumulative.  What you learn yesterday helps you take a step further today. Learning math through high school and into college, problems are presented in a progressive manner such that material builds on previously taught topics. Arithmetic leads into algebra, which then leads into geometry and trigonometry. Geometry helps understand linear (matrix) algebra. Calculus turns out to be nothing more than making linear approximations to nonlinear functions.  

A difficult task confronting people who want to advance their methodological skills but who were not math majors is that most advanced topics are presented by other online resources, and even within graduate programs, as stand-alone lessons.  Trying to piece these topics together to generate a coherent explanation that builds up from first principles is difficult. The intent of this book is to offer, within a single resource, a series of lessons that are meant to build cumulatively, the way math is best taught.  Topics such as logistic regression then do not have to shy away from discussing asymptotic results (why we know the model works in large samples, but maybe not small samples), and the reader does not have to stare cross-eyed at discussions of probability limits.  By beginning with a comprehensive covering of first principles (not just an unhelpfully pithy "mathematical appendix"), the meat of multivariate statistics, econometrics, and machine learning can be attacked in more than just a superficial manner.

You can feel confident that you have mastered a math or modeling problem when you can explain it in four different ways:

1. With pictures
2. With words
3. With equations
4. With code

This book will focus on the first three of these.  Once you can picture, explain, and formalize, the coding comes naturally.  At that point you can sign up for a "learn data science in a week" and maybe get something out of it.  That hard part is getting the intuition and comfort with translating the intuition into math.

Solving mathematical problems uses a different part of the brain than learning a language, yet communicating mathematical results requires the ability to recognize certain symbols and formulas as words.  When we learned arithmetic, we memorized words to accompany operators.  Imagine trying to solve a problem like $2 + 10 \div 5$ without having words for "plus" and "divided by".  A methodologist who has spent enough time working with formulas soon learns to read semmingly complex mathematical expressions as words.  They see an expression like $\frac{e^{-\lambda}\lambda^x}{x!}$ and read it as "Poisson!"  Or they see $\int xp(x)dx$ and read it as "Expected value!"  Or they see $\frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$ and declare, "Normal!"  For this reason, each chapter will end with a set of "vocabulary" terms that will sometimes be words, sometimes symbols, and sometimes formulas.  It is highly encouraged that you memorize these.  As the material builds, the task of learning will be much, much easier if the symbols and formulas can be read as words, even as they are being used to perform logical calculations.  This chapter ends simply with the Greek alphabet.  Not knowing that $\Lambda$, for example, is pronounced _lambda_ makes it difficult to follow any logical sequence of operations in which a parameter is written with the Greek letter.


## Delimiting the Book's Scope Using Set Operators

## Organization of This Book

This book will be a long slog, but if you are a Ph.D. student you can work through it in the months it takes your advisor to read your latest dissertation draft. The pre-requisites are high school algebra, though it is also assumed you have read enough research articles to know what regression is and have possibly wondered how to prove the existence of a Nash equilibrium.  

The first section of the book covers the material usually found in a first course in calculus.  Functions, which form the core of all quantitative modeling (be it data driven or game theoretic), come first.  This is followed by a discussion of limits and the convergence of sequences.  Finally, limits will be used to derive the core results from single variable calculus: finding the rate of change in a curved line and finding the area under a curved surface in one dimension.  Along the way you will be exposed to common concepts such as step functions, the formula for drawing a normal curve, how to prove the folk theorem in repeated Prisoners Dilemmas, and interpreting regression models that include a polynomial or log transformation. 

The next section turns to linear algebra.  In most "math for economists" types of books, matrices are discussed in the most uninteresting and unintuitive manner possible.  The steps to multiple matrices are discussed, with vectors taught as special cases of matrices rather than as interesting in their own right. Then a matrix solution for finding the least squares linear regression estimates are presented. Such books maybe add a section on the geometric interpretation of vectors without a clear mapping of geometry onto how matrices are used in multivariate modeling, and the least squares solution is not even how your computer solves the problem. You ultimately walk away hating matrices and feel your blood pressure raise everytime you see them referenced in textbooks, a problem made worse by the fact that they are usually referred to with bold, capital letters that look like they are screaming at you.

This book takes a different approach. The geometric interpretation is presented first, which makes it possible to then understand matrices as altering a given space.  The geometry makes, in my opinion, the material more engaging, but it also helps the reader understand how matrices generalize to all sorts of topics.  The "math for economists" approach does not make it clear, for example, what is going on with principal components analysis, despite this being a very common methodology in both statistics and unsupervised machine learning.

Armed with an understanding of matrices and their implications for navigating multidimensional spaces, the third section returns to calculus and discusses generalizes of the first section to situations in which multiple variables - indpendent, dependent, or both - are involved.  This makes it possible to understand how to interpret things like interaction terms in regressions, the rate of change in predicted probabilities from logistic regression, and even how back-propagation works in neural networks.  In addition, complicated models such as nonlinear mixed models begin to make more sense, as we can define what it means to "integrate out" random effects.  

This is then followed by a section on probability theory.  Cumulative probability functions, probability density functions, and how to derive one from the other are discussed and illustrated with survival models.  At this point, with an understanding of applying integrals to continuous random variables, we can also dig into Bayesian statistics. Topics such as the Central Limit Theorem and other important results for linear models are also covered.

We then turn to statistics.  Although linear regression is no longer considered adequate for uncovering treatment effects from observational data or providing reliable predictions in machine learning, it is still the baseline from which other methods are often derived.  We will derive the estimator and discuss computationally efficient methods for getting the model results.  Assumptions and how to test them are discussed.

Before moving into nonlinear statistical models, a subsequent section covers "multivariate" linear models, where the "multivariate" refers to the number of _dependent_ variables.  This includes methods like PCA, exploratory factor analysis, multivariate analysis of variance (MANOVA), and multivariate regression.  Despite being linear models, these methods can seem opaque to somebody not used to thinking in terms of linear algebra.  Fortunately, the concepts covered in the linear algebra section will make the material far easier to penetrate.

Extensions to the linear model are presented next.  First, the linear mixed model is introduced, which allows for the inclusion of random effects to appropriately model such things as clustered and longitudinal data.  This is followed by a chapter on logistic regression and its extensions, a chapter on models for count outcomes, and finally how all of these come together under umbrella of _generalized linear models_.  The logical extension, the _generalized linear mixed model_, is presented last.

Generalized linear (mixed) models are still widely used, though - like with regression - their utility for identifying treatment effects in observational data or making out-of-sample predictions is now widely questions.  The next part of the book discusses causal inference with a particular focus on the potential outcomes framework.  Multiple motivations for using propensity scores instead of regression are described, and the use of propensity scores for matching and re-weighting treatment groups are discussed. Other "identification strategies" for treatment effects, including regression-discontinuity, fixed effects regression, and instrumental variables are then discussed in turn.

The final section of the book turns to machine learning.  Whereas statistics is focused on treatment effects, or what happens when one turns a single dial by itself, machine learning considers the joint, often nonlinear and interactive effects of multiple predictors (called "features" in the ML literature). The bias-variance trade-off and dangers of model overfitting are discussed first, with a comparison of the predictive accuracy of traditional logistic regression versus its regularized variants (ridge and LASSO).

This chapter will be followed by an explanation, working from first principles developed at the start of the book, of how tree-based methods (random forests, gradient boosted machines), support vector machines, and shallow neural nets work.  This is followed by a chapter on deep learning models for image classification and current topics in artificial intelligence.

It is hoped that working through this book will be an experience akin to reading Thomas Pynchon's _Gravity's Rainbow_.  Sometimes it will be fascinating.  Sometimes beguiling.  Sometimes amusing. Sometimes terribly boring. But when it is finished you can feel a sense of joy and accomplishment, and you can impress (annoy?) your friends bragging about having completed it. Most people are not willing to put in the effort it takes to truly understand the mathematical underpinnings of data science. But you are not content to simply take output from a computer algorithm as Gospel truth.  You don't just want to complete a task, you want to _understand_ it.  Take pride in the fact that you are willing to put in the effort to not skip the hard parts.  Take pride in the fact that you are stepping out of your comfort zone and approaching your career in a manner that is challenging, rewarding, and decidely _not_ $\frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$.  



